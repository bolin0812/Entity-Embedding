# Entity Embedding
Embedding methods transform high-dimensional sparse vectors into low-dimensional vectors. 

|    Research Paper    |  Key Point  |
|  :---------  | :------:  |
|  [Distributed Representations of Sentences and Documents](https://arxiv.org/pdf/1405.4053v2.pdf)  |This study proposes Paragraph based Vector, an unsupervised algorithm that learns ﬁxed-length feature representations from variable-length pieces of texts, such as sentences, paragraphs, and documents. The algorithm represents each document by a dense vector which is trained to predict words in the document. Its construction potentially overcome the weaknesses of bag-of words models. | 
|  [Doc2vec paragraph embeddings](https://radimrehurek.com/gensim/models/doc2vec.html)  | Applied doc2vec functions in genism to create lower dimensional transaction vector. It has been used for DBSCAN with cosine similarity metric. |
|  [Distributed Representations of Words and Phrases and their Compositionality](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf)  | This study presents extensions of Skip-gram that improve both the quality of the vectors and the training speed by subsampling of the frequent words. This study describes a simple alternative to the hierarchical softmax called negative sampling. |
|  [Entity Embedding of Categorical Variables](https://arxiv.org/pdf/1604.06737.pdf)  | Map categorical variables in a function approximation problem into Euclidean spaces, which are the entity embeddings of the categorical variables. The mapping is learned by a neural network during the standard supervised training process. It not only reduces memory usage and speeds up neural networks compared with one-hot encoding, but more importantly by mapping similar values close to each other in the embedding space it reveals the intrinsic properties of the categorical variables. |
|  [Meta-Prod2Vec - Product Embedding Using Side-Information for Recommendation](https://arxiv.org/pdf/1607.07326.pdf)  | Meta-Prod2vec computes item similarities for recommendation that leverages existing item metadata. Such scenarios are frequently encountered in applications such as content recommendation, ad targeting and web search. This method leverages past user interactions with items and their attributes to compute low-dimensional embeddings of items. |
|  [StarSpace: Embed All The Things!](https://arxiv.org/pdf/1709.03856.pdf)  | StarSpace solves a wide variety of problems: labeling tasks such as text classiﬁcation, ranking tasks such as information retrieval/web search, collaborative ﬁltering-based or content-based recommendation, embedding of multirelational graphs, and learning word, sentence or document level embeddings. |
|  [Wide & Deep Learning for Recommender Systems](https://arxiv.org/pdf/1606.07792.pdf)  | Wide & Deep learning—jointly trained wide linear models and deep neural networks—to combine the beneﬁts of memorization and generalization for recommender systems. Deep learning could leverage embedding layers for entity variables. |
|  [Deep Learning for Recommendation System](https://www.slideshare.net/%E2%80%A6/deep-learning-for-recommender-systems-recsys2017-tutorial)  | Applications of deep learning algorithms in recommendation system. Multiple methods are introduced to do entity embedding. |

